{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097c9bfc",
   "metadata": {},
   "source": [
    "# Document Forgery Detection - Data Exploration\n",
    "\n",
    "This notebook focuses on exploring and understanding document image datasets for forgery detection.\n",
    "\n",
    "## Objectives:\n",
    "- Load and examine document images\n",
    "- Analyze image properties and metadata\n",
    "- Identify potential forgery indicators\n",
    "- Visualize data distributions and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6830953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image, ExifTags\n",
    "from scipy import stats\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from features.build_features import DocumentFeatureExtractor\n",
    "from visualization.visualize import DocumentForgeryVisualizer\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5b14d",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = '../data/raw'\n",
    "data_dir = Path(DATA_PATH)\n",
    "\n",
    "# Find all image files\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
    "image_files = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(data_dir.glob(f'**/*{ext}'))\n",
    "    image_files.extend(data_dir.glob(f'**/*{ext.upper()}'))\n",
    "\n",
    "print(f\"Found {len(image_files)} image files\")\n",
    "\n",
    "if len(image_files) > 0:\n",
    "    print(f\"\\nSample files:\")\n",
    "    for i, img_path in enumerate(image_files[:10]):\n",
    "        print(f\"  {i+1}. {img_path}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No images found. Please add images to the data/raw directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab74aa",
   "metadata": {},
   "source": [
    "## 2. Image Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_properties(image_files, max_images=50):\n",
    "    \"\"\"Analyze basic properties of images\"\"\"\n",
    "    properties = []\n",
    "    \n",
    "    for img_path in image_files[:max_images]:\n",
    "        try:\n",
    "            # Load image\n",
    "            img = Image.open(img_path)\n",
    "            img_cv = cv2.imread(str(img_path))\n",
    "            \n",
    "            # Basic properties\n",
    "            prop = {\n",
    "                'filename': img_path.name,\n",
    "                'path': str(img_path),\n",
    "                'width': img.width,\n",
    "                'height': img.height,\n",
    "                'aspect_ratio': img.width / img.height,\n",
    "                'file_size_kb': img_path.stat().st_size / 1024,\n",
    "                'format': img.format,\n",
    "                'mode': img.mode,\n",
    "                'has_exif': bool(img._getexif())\n",
    "            }\n",
    "            \n",
    "            # Determine class from path/filename\n",
    "            path_str = str(img_path).lower()\n",
    "            if any(word in path_str for word in ['authentic', 'real', 'original', 'genuine']):\n",
    "                prop['class'] = 'authentic'\n",
    "            elif any(word in path_str for word in ['forged', 'fake', 'manipulated', 'tampered']):\n",
    "                prop['class'] = 'forged'\n",
    "            else:\n",
    "                prop['class'] = 'unknown'\n",
    "            \n",
    "            # Color statistics\n",
    "            if img_cv is not None:\n",
    "                gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "                prop['mean_intensity'] = np.mean(gray)\n",
    "                prop['std_intensity'] = np.std(gray)\n",
    "                prop['brightness'] = np.mean(img_cv)\n",
    "            \n",
    "            properties.append(prop)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(properties)\n",
    "\n",
    "if len(image_files) > 0:\n",
    "    print(\"Analyzing image properties...\")\n",
    "    df_props = analyze_image_properties(image_files)\n",
    "    \n",
    "    print(f\"\\n📊 Dataset Summary:\")\n",
    "    print(f\"Total images analyzed: {len(df_props)}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df_props['class'].value_counts())\n",
    "    \n",
    "    print(f\"\\nImage dimensions:\")\n",
    "    print(df_props[['width', 'height']].describe())\n",
    "else:\n",
    "    df_props = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec001358",
   "metadata": {},
   "source": [
    "## 3. Visual Analysis of Image Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_props.empty:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Image dimensions\n",
    "    axes[0, 0].scatter(df_props['width'], df_props['height'], \n",
    "                      c=df_props['class'].map({'authentic': 'blue', 'forged': 'red', 'unknown': 'gray'}),\n",
    "                      alpha=0.6)\n",
    "    axes[0, 0].set_xlabel('Width (pixels)')\n",
    "    axes[0, 0].set_ylabel('Height (pixels)')\n",
    "    axes[0, 0].set_title('Image Dimensions by Class')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. File size distribution\n",
    "    for class_name in df_props['class'].unique():\n",
    "        if class_name != 'unknown':\n",
    "            class_data = df_props[df_props['class'] == class_name]\n",
    "            axes[0, 1].hist(class_data['file_size_kb'], bins=20, alpha=0.6, label=class_name)\n",
    "    axes[0, 1].set_xlabel('File Size (KB)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('File Size Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Aspect ratio\n",
    "    sns.boxplot(data=df_props, x='class', y='aspect_ratio', ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('Aspect Ratio by Class')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Mean intensity\n",
    "    if 'mean_intensity' in df_props.columns:\n",
    "        sns.boxplot(data=df_props, x='class', y='mean_intensity', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Mean Intensity by Class')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Standard deviation of intensity\n",
    "    if 'std_intensity' in df_props.columns:\n",
    "        sns.boxplot(data=df_props, x='class', y='std_intensity', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Intensity Std Dev by Class')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 6. Format distribution\n",
    "    format_counts = df_props['format'].value_counts()\n",
    "    axes[1, 2].pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 2].set_title('Image Format Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee881e",
   "metadata": {},
   "source": [
    "## 4. Feature-based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa92327",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(image_files) > 5:\n",
    "    print(\"Extracting advanced features for analysis...\")\n",
    "    \n",
    "    # Extract features from a sample of images\n",
    "    extractor = DocumentFeatureExtractor()\n",
    "    sample_files = image_files[:20]  # Analyze first 20 images\n",
    "    \n",
    "    features_data = []\n",
    "    for img_path in sample_files:\n",
    "        print(f\"Processing {img_path.name}...\")\n",
    "        features = extractor.extract_all_features(str(img_path))\n",
    "        \n",
    "        if features:\n",
    "            features['filename'] = img_path.name\n",
    "            features['filepath'] = str(img_path)\n",
    "            \n",
    "            # Determine class\n",
    "            path_str = str(img_path).lower()\n",
    "            if any(word in path_str for word in ['authentic', 'real', 'original']):\n",
    "                features['class'] = 'authentic'\n",
    "            elif any(word in path_str for word in ['forged', 'fake', 'manipulated']):\n",
    "                features['class'] = 'forged'\n",
    "            else:\n",
    "                features['class'] = 'unknown'\n",
    "            \n",
    "            features_data.append(features)\n",
    "    \n",
    "    if features_data:\n",
    "        df_features = pd.DataFrame(features_data)\n",
    "        print(f\"\\n✅ Features extracted for {len(df_features)} images\")\n",
    "        print(f\"Feature count: {len(df_features.columns) - 3}\")\n",
    "        \n",
    "        # Show feature statistics by class\n",
    "        numeric_cols = df_features.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"\\n📊 Feature summary by class:\")\n",
    "            summary = df_features.groupby('class')[numeric_cols].mean()\n",
    "            print(summary.round(4))\n",
    "    else:\n",
    "        df_features = pd.DataFrame()\n",
    "        print(\"❌ No features could be extracted\")\n",
    "else:\n",
    "    df_features = pd.DataFrame()\n",
    "    print(\"⚠️ Not enough images for feature analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd73c6",
   "metadata": {},
   "source": [
    "## 5. Advanced Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb95a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_features.empty and len(df_features) > 5:\n",
    "    # Select interesting features for visualization\n",
    "    interesting_features = [\n",
    "        'mean', 'std', 'entropy', 'sobel_mean', 'canny_edge_density',\n",
    "        'laplacian_var', 'fft_energy', 'lbp_contrast', 'glcm_contrast_mean'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available features\n",
    "    available_features = [f for f in interesting_features if f in df_features.columns]\n",
    "    \n",
    "    if len(available_features) >= 4:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Box plots for each feature\n",
    "        for i, feature in enumerate(available_features[:4]):\n",
    "            row, col = i // 2, i % 2\n",
    "            \n",
    "            if len(df_features['class'].unique()) > 1:\n",
    "                sns.boxplot(data=df_features, x='class', y=feature, ax=axes[row, col])\n",
    "            else:\n",
    "                df_features[feature].hist(ax=axes[row, col], bins=10)\n",
    "            \n",
    "            axes[row, col].set_title(f'{feature} Distribution')\n",
    "            axes[row, col].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Correlation analysis\n",
    "        if len(available_features) > 1:\n",
    "            print(\"\\n🔍 Feature Correlation Analysis:\")\n",
    "            corr_matrix = df_features[available_features].corr()\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                       square=True, fmt='.2f')\n",
    "            plt.title('Feature Correlation Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️ Not enough features for advanced visualization\")\n",
    "        print(f\"Available features: {list(df_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ab91c",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef68d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_features.empty and len(df_features) > 5:\n",
    "    # Statistical tests to find discriminative features\n",
    "    print(\"🧮 Statistical Analysis of Features:\")\n",
    "    \n",
    "    # Filter to classes with enough samples\n",
    "    class_counts = df_features['class'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "    \n",
    "    if len(valid_classes) >= 2:\n",
    "        df_filtered = df_features[df_features['class'].isin(valid_classes)]\n",
    "        numeric_features = df_filtered.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        # Perform t-tests or Mann-Whitney U tests\n",
    "        significant_features = []\n",
    "        \n",
    "        for feature in numeric_features:\n",
    "            try:\n",
    "                groups = [df_filtered[df_filtered['class'] == cls][feature].values \n",
    "                         for cls in valid_classes]\n",
    "                \n",
    "                # Remove NaN values\n",
    "                groups = [g[~np.isnan(g)] for g in groups]\n",
    "                \n",
    "                if all(len(g) > 1 for g in groups):\n",
    "                    # Use Mann-Whitney U test (non-parametric)\n",
    "                    statistic, p_value = stats.mannwhitneyu(groups[0], groups[1], \n",
    "                                                           alternative='two-sided')\n",
    "                    \n",
    "                    if p_value < 0.05:  # Significant difference\n",
    "                        significant_features.append({\n",
    "                            'feature': feature,\n",
    "                            'p_value': p_value,\n",
    "                            'statistic': statistic\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                pass  # Skip features that cause errors\n",
    "        \n",
    "        if significant_features:\n",
    "            # Sort by p-value\n",
    "            significant_features.sort(key=lambda x: x['p_value'])\n",
    "            \n",
    "            print(f\"\\n📈 Most discriminative features (p < 0.05):\")\n",
    "            for i, feat in enumerate(significant_features[:10]):\n",
    "                print(f\"{i+1:2d}. {feat['feature']:<25} (p = {feat['p_value']:.4f})\")\n",
    "                \n",
    "            # Visualize top discriminative features\n",
    "            if len(significant_features) >= 2:\n",
    "                top_features = [f['feature'] for f in significant_features[:4]]\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for i, feature in enumerate(top_features):\n",
    "                    sns.boxplot(data=df_filtered, x='class', y=feature, ax=axes[i])\n",
    "                    axes[i].set_title(f'{feature}\\n(p = {significant_features[i][\"p_value\"]:.4f})')\n",
    "                    axes[i].tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                plt.suptitle('Most Discriminative Features', fontsize=16, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"⚠️ No statistically significant features found\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️ Need at least 2 samples per class for statistical analysis\")\n",
    "else:\n",
    "    print(\"⚠️ No feature data available for statistical analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6579d5",
   "metadata": {},
   "source": [
    "## 7. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"📁 Total images found: {len(image_files)}\")\n",
    "\n",
    "if not df_props.empty:\n",
    "    print(f\"\\n📊 Basic Properties:\")\n",
    "    print(f\"   - Image formats: {', '.join(df_props['format'].unique())}\")\n",
    "    print(f\"   - Size range: {df_props['file_size_kb'].min():.1f} - {df_props['file_size_kb'].max():.1f} KB\")\n",
    "    print(f\"   - Resolution range: {df_props['width'].min()}x{df_props['height'].min()} to {df_props['width'].max()}x{df_props['height'].max()}\")\n",
    "    \n",
    "    class_dist = df_props['class'].value_counts()\n",
    "    print(f\"\\n🏷️ Class Distribution:\")\n",
    "    for class_name, count in class_dist.items():\n",
    "        print(f\"   - {class_name}: {count} images ({count/len(df_props)*100:.1f}%)\")\n",
    "\n",
    "if not df_features.empty:\n",
    "    print(f\"\\n🔧 Advanced Features:\")\n",
    "    print(f\"   - Features extracted: {len(df_features.columns) - 3}\")\n",
    "    print(f\"   - Samples analyzed: {len(df_features)}\")\n",
    "    \n",
    "    if 'significant_features' in locals() and significant_features:\n",
    "        print(f\"   - Discriminative features found: {len(significant_features)}\")\n",
    "        print(f\"   - Most discriminative: {significant_features[0]['feature']} (p={significant_features[0]['p_value']:.4f})\")\n",
    "\n",
    "print(\"\\n💡 Key Insights:\")\n",
    "\n",
    "if not df_props.empty:\n",
    "    # Analyze potential issues\n",
    "    insights = []\n",
    "    \n",
    "    if df_props['class'].value_counts().get('unknown', 0) > 0:\n",
    "        insights.append(\"Some images couldn't be automatically classified - consider better file organization\")\n",
    "    \n",
    "    if len(df_props['format'].unique()) > 2:\n",
    "        insights.append(\"Multiple image formats detected - consider standardizing\")\n",
    "    \n",
    "    size_cv = df_props['file_size_kb'].std() / df_props['file_size_kb'].mean()\n",
    "    if size_cv > 1.0:\n",
    "        insights.append(\"High variation in file sizes - may indicate different compression levels\")\n",
    "    \n",
    "    aspect_ratios = df_props['aspect_ratio'].unique()\n",
    "    if len(aspect_ratios) > 5:\n",
    "        insights.append(\"Many different aspect ratios - documents may need standardized preprocessing\")\n",
    "    \n",
    "    if insights:\n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            print(f\"   {i}. {insight}\")\n",
    "    else:\n",
    "        print(\"   - Dataset appears well-structured for analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"   - Add document images to data/raw/ directory to begin analysis\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"   1. Ensure balanced dataset with enough samples per class\")\n",
    "print(\"   2. Standardize image preprocessing (size, format, etc.)\")\n",
    "print(\"   3. Extract features for all images in the dataset\")\n",
    "print(\"   4. Train models using the most discriminative features\")\n",
    "print(\"   5. Validate performance on held-out test data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
