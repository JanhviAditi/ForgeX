{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700062a1",
   "metadata": {},
   "source": [
    "# Document Forgery Detection - Complete Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for document forgery detection using computer vision and machine learning techniques.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading and Exploration](#data-loading)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Feature Extraction](#feature-extraction)\n",
    "5. [Model Training](#model-training)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Making Predictions](#predictions)\n",
    "8. [Visualization and Analysis](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eec2f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Add src to path for importing our modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data.make_dataset import load_image, create_dataset_structure\n",
    "from features.build_features import DocumentFeatureExtractor, extract_features_from_directory\n",
    "from models.train_model import DocumentForgeryDetector\n",
    "from models.predict_model import DocumentForgeryPredictor\n",
    "from visualization.visualize import DocumentForgeryVisualizer, create_comprehensive_report\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b535578",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration <a id=\"data-loading\"></a>\n",
    "\n",
    "First, let's load and explore our document dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "RAW_DATA_PATH = '../data/raw'\n",
    "PROCESSED_DATA_PATH = '../data/processed'\n",
    "MODELS_PATH = '../models'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path(RAW_DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(PROCESSED_DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODELS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Raw data path: {RAW_DATA_PATH}\")\n",
    "print(f\"Processed data path: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Models path: {MODELS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have data in the raw directory\n",
    "raw_path = Path(RAW_DATA_PATH)\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
    "\n",
    "image_files = []\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(raw_path.glob(f'**/*{ext}'))\n",
    "    image_files.extend(raw_path.glob(f'**/*{ext.upper()}'))\n",
    "\n",
    "print(f\"Found {len(image_files)} image files in the raw data directory\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(\"\\n⚠️ No images found in the raw data directory.\")\n",
    "    print(\"To use this notebook with real data:\")\n",
    "    print(f\"1. Place your document images in: {RAW_DATA_PATH}\")\n",
    "    print(\"2. Organize them in subfolders like 'authentic' and 'forged'\")\n",
    "    print(\"3. Or name files with keywords like 'fake', 'authentic', etc.\")\n",
    "    print(\"\\n📝 For now, we'll create synthetic sample data for demonstration.\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    sample_data = True\n",
    "else:\n",
    "    sample_data = False\n",
    "    # Display some sample images if available\n",
    "    print(f\"\\n📁 Sample image files:\")\n",
    "    for i, img_path in enumerate(image_files[:5]):\n",
    "        print(f\"  {i+1}. {img_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e6f26",
   "metadata": {},
   "source": [
    "### Creating Sample Data (if no real data is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f55efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_data:\n",
    "    # Create synthetic sample images for demonstration\n",
    "    print(\"Creating synthetic sample data...\")\n",
    "    \n",
    "    # Create sample authentic documents (clean, structured)\n",
    "    authentic_dir = Path(RAW_DATA_PATH) / 'authentic'\n",
    "    forged_dir = Path(RAW_DATA_PATH) / 'forged'\n",
    "    \n",
    "    authentic_dir.mkdir(exist_ok=True)\n",
    "    forged_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate sample images\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(50):\n",
    "        # Authentic documents - more structured, less noise\n",
    "        img = np.random.randint(200, 255, (300, 400, 3), dtype=np.uint8)  # Light background\n",
    "        # Add some structure (lines, text-like patterns)\n",
    "        for j in range(5):\n",
    "            y = 50 + j * 40\n",
    "            cv2.line(img, (20, y), (380, y), (0, 0, 0), 2)  # Horizontal lines\n",
    "        \n",
    "        # Add some noise\n",
    "        noise = np.random.normal(0, 10, img.shape)\n",
    "        img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        cv2.imwrite(str(authentic_dir / f'authentic_doc_{i:03d}.png'), img)\n",
    "    \n",
    "    for i in range(50):\n",
    "        # Forged documents - more irregular, artifacts\n",
    "        img = np.random.randint(180, 255, (300, 400, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add irregular patterns (forgery artifacts)\n",
    "        for j in range(5):\n",
    "            y = 50 + j * 40 + np.random.randint(-5, 5)  # Slight misalignment\n",
    "            cv2.line(img, (20, y), (380, y), (0, 0, 0), np.random.randint(1, 4))  # Variable thickness\n",
    "        \n",
    "        # Add compression artifacts\n",
    "        for _ in range(10):\n",
    "            x, y = np.random.randint(0, 400), np.random.randint(0, 300)\n",
    "            cv2.rectangle(img, (x, y), (x+8, y+8), (np.random.randint(0, 100),)*3, -1)\n",
    "        \n",
    "        # Add more noise\n",
    "        noise = np.random.normal(0, 20, img.shape)\n",
    "        img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        cv2.imwrite(str(forged_dir / f'forged_doc_{i:03d}.png'), img)\n",
    "    \n",
    "    print(\"✅ Sample data created successfully!\")\n",
    "    print(f\"   - 50 authentic documents in {authentic_dir}\")\n",
    "    print(f\"   - 50 forged documents in {forged_dir}\")\n",
    "    \n",
    "    # Update image files list\n",
    "    image_files = list(authentic_dir.glob('*.png')) + list(forged_dir.glob('*.png'))\n",
    "    print(f\"\\n📊 Total images: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d37c9",
   "metadata": {},
   "source": [
    "### Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample images\n",
    "if len(image_files) > 0:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Show authentic images\n",
    "    authentic_files = [f for f in image_files if 'authentic' in str(f).lower()][:4]\n",
    "    for i, img_path in enumerate(authentic_files):\n",
    "        if i < 4:\n",
    "            img = Image.open(img_path)\n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].set_title(f'Authentic: {img_path.name}', fontsize=10)\n",
    "            axes[0, i].axis('off')\n",
    "    \n",
    "    # Show forged images\n",
    "    forged_files = [f for f in image_files if 'forged' in str(f).lower() or 'fake' in str(f).lower()][:4]\n",
    "    for i, img_path in enumerate(forged_files):\n",
    "        if i < 4:\n",
    "            img = Image.open(img_path)\n",
    "            axes[1, i].imshow(img)\n",
    "            axes[1, i].set_title(f'Forged: {img_path.name}', fontsize=10)\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📊 Dataset Summary:\")\n",
    "    print(f\"   - Total images: {len(image_files)}\")\n",
    "    print(f\"   - Authentic images: {len(authentic_files)}\")\n",
    "    print(f\"   - Forged images: {len(forged_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b07aae",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a id=\"preprocessing\"></a>\n",
    "\n",
    "Now let's preprocess the data and create a structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dada10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured dataset\n",
    "if len(image_files) > 0:\n",
    "    print(\"Creating structured dataset...\")\n",
    "    \n",
    "    # Use the create_dataset_structure function\n",
    "    create_dataset_structure(\n",
    "        input_dir=RAW_DATA_PATH,\n",
    "        output_dir=PROCESSED_DATA_PATH,\n",
    "        train_split=0.7,\n",
    "        val_split=0.2,\n",
    "        test_split=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Dataset structure created!\")\n",
    "    \n",
    "    # Load and display the metadata\n",
    "    metadata_file = Path(PROCESSED_DATA_PATH) / 'dataset_metadata.csv'\n",
    "    if metadata_file.exists():\n",
    "        df_metadata = pd.read_csv(metadata_file)\n",
    "        print(f\"\\n📊 Dataset Statistics:\")\n",
    "        print(df_metadata.groupby(['split', 'class']).size().unstack(fill_value=0))\n",
    "        \n",
    "        # Visualize data distribution\n",
    "        visualizer = DocumentForgeryVisualizer()\n",
    "        visualizer.plot_data_distribution(df_metadata, target_column='class')\n",
    "    else:\n",
    "        print(\"⚠️ Metadata file not found\")\n",
    "else:\n",
    "    print(\"⚠️ No images available for preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5b56a",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction <a id=\"feature-extraction\"></a>\n",
    "\n",
    "Extract comprehensive features from the document images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "feature_extractor = DocumentFeatureExtractor(image_size=(224, 224))\n",
    "\n",
    "# Extract features from training data\n",
    "train_dir = Path(PROCESSED_DATA_PATH) / 'train'\n",
    "features_file = Path(PROCESSED_DATA_PATH) / 'features.csv'\n",
    "\n",
    "if train_dir.exists() and len(list(train_dir.glob('**/*.png'))) > 0:\n",
    "    print(\"Extracting features from training data...\")\n",
    "    \n",
    "    extract_features_from_directory(\n",
    "        input_dir=str(train_dir),\n",
    "        output_file=str(features_file)\n",
    "    )\n",
    "    \n",
    "    # Load and analyze features\n",
    "    if features_file.exists():\n",
    "        df_features = pd.read_csv(features_file)\n",
    "        print(f\"\\n✅ Features extracted successfully!\")\n",
    "        print(f\"   - Total samples: {len(df_features)}\")\n",
    "        print(f\"   - Feature count: {len(df_features.columns) - 2}\")\n",
    "        \n",
    "        # Determine class from filepath\n",
    "        df_features['class'] = df_features['filepath'].apply(\n",
    "            lambda x: 'authentic' if 'authentic' in str(x) else 'forged'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n📊 Class distribution:\")\n",
    "        print(df_features['class'].value_counts())\n",
    "        \n",
    "        # Display sample features\n",
    "        print(f\"\\n🔍 Sample features (first 10):\")\n",
    "        feature_cols = [col for col in df_features.columns if col not in ['filename', 'filepath', 'class']]\n",
    "        print(df_features[feature_cols[:10]].describe())\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Feature extraction failed\")\n",
    "        df_features = None\n",
    "else:\n",
    "    print(\"⚠️ No training data found for feature extraction\")\n",
    "    df_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2377c",
   "metadata": {},
   "source": [
    "### Visualize Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_features is not None and len(df_features) > 0:\n",
    "    # Select some important features to visualize\n",
    "    important_features = [\n",
    "        'mean', 'std', 'entropy', 'sobel_mean', 'canny_edge_density',\n",
    "        'laplacian_var', 'fft_energy', 'lbp_contrast'\n",
    "    ]\n",
    "    \n",
    "    # Filter features that exist in our dataset\n",
    "    available_features = [f for f in important_features if f in df_features.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        print(f\"Visualizing {len(available_features)} important features...\")\n",
    "        \n",
    "        visualizer = DocumentForgeryVisualizer()\n",
    "        visualizer.plot_feature_distribution(\n",
    "            df_features, \n",
    "            features=available_features[:9],  # Show up to 9 features\n",
    "            target_column='class'\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚠️ No matching features found for visualization\")\n",
    "        print(f\"Available features: {list(df_features.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81655bd9",
   "metadata": {},
   "source": [
    "## 5. Model Training <a id=\"model-training\"></a>\n",
    "\n",
    "Train different types of models for document forgery detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1f34c",
   "metadata": {},
   "source": [
    "### 5.1 Traditional Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf399c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_features is not None and len(df_features) > 10:\n",
    "    print(\"Training traditional ML model (Random Forest)...\")\n",
    "    \n",
    "    # Initialize detector for traditional ML\n",
    "    ml_detector = DocumentForgeryDetector(model_type='traditional_ml', random_state=42)\n",
    "    \n",
    "    # Load data using the features CSV\n",
    "    X, y = ml_detector.load_data(str(features_file))\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_processed, y_processed = ml_detector.preprocess_data(X, y)\n",
    "    \n",
    "    # Train model\n",
    "    ml_results = ml_detector.train_model(\n",
    "        X_processed, y_processed,\n",
    "        model_name='random_forest',\n",
    "        use_grid_search=False  # Set to True for better performance (takes longer)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Traditional ML Model Training Results:\")\n",
    "    print(f\"   - Training Accuracy: {ml_results['train_accuracy']:.4f}\")\n",
    "    print(f\"   - Test Accuracy: {ml_results['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    ml_model_path = Path(MODELS_PATH) / 'random_forest_model.joblib'\n",
    "    ml_detector.save_model(str(ml_model_path), metadata=ml_results)\n",
    "    \n",
    "    print(f\"   - Model saved to: {ml_model_path}\")\n",
    "    \n",
    "    # Display feature importance if available\n",
    "    if hasattr(ml_detector.model, 'feature_importances_'):\n",
    "        feature_cols = [col for col in df_features.columns if col not in ['filename', 'filepath', 'class']]\n",
    "        visualizer = DocumentForgeryVisualizer()\n",
    "        visualizer.plot_feature_importance(\n",
    "            feature_cols, \n",
    "            ml_detector.model.feature_importances_,\n",
    "            top_k=15\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Insufficient data for training traditional ML model\")\n",
    "    ml_detector = None\n",
    "    ml_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167dbef5",
   "metadata": {},
   "source": [
    "### 5.2 Deep Learning Model (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bda059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have enough processed images for CNN training\n",
    "processed_images = list(Path(PROCESSED_DATA_PATH).glob('train/**/*.png'))\n",
    "\n",
    "if len(processed_images) > 10:\n",
    "    print(f\"Training CNN model with {len(processed_images)} images...\")\n",
    "    \n",
    "    # Initialize CNN detector\n",
    "    cnn_detector = DocumentForgeryDetector(model_type='cnn', random_state=42)\n",
    "    \n",
    "    try:\n",
    "        # Load image data\n",
    "        X_images, y_images = cnn_detector.load_data(str(Path(PROCESSED_DATA_PATH) / 'train'))\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_img_processed, y_img_processed = cnn_detector.preprocess_data(X_images, y_images)\n",
    "        \n",
    "        print(f\"   - Image shape: {X_img_processed.shape}\")\n",
    "        print(f\"   - Labels shape: {y_img_processed.shape}\")\n",
    "        \n",
    "        # Train model (reduced epochs for demo)\n",
    "        cnn_results = cnn_detector.train_model(\n",
    "            X_img_processed, y_img_processed,\n",
    "            epochs=10,  # Reduced for demo - use 50+ for real training\n",
    "            batch_size=16\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ CNN Model Training Results:\")\n",
    "        print(f\"   - Final Validation Accuracy: {cnn_results['final_accuracy']:.4f}\")\n",
    "        print(f\"   - Final Loss: {cnn_results['final_loss']:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        cnn_model_path = Path(MODELS_PATH) / 'cnn_model.h5'\n",
    "        cnn_detector.save_model(str(cnn_model_path), metadata=cnn_results)\n",
    "        \n",
    "        print(f\"   - Model saved to: {cnn_model_path}\")\n",
    "        \n",
    "        # Plot training history\n",
    "        cnn_detector.plot_training_history()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CNN training failed: {str(e)}\")\n",
    "        print(\"This might be due to insufficient memory or missing dependencies.\")\n",
    "        cnn_detector = None\n",
    "        cnn_results = None\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Insufficient images for CNN training\")\n",
    "    print(\"CNN models typically need hundreds or thousands of images for good performance.\")\n",
    "    cnn_detector = None\n",
    "    cnn_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954d9c6",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation <a id=\"evaluation\"></a>\n",
    "\n",
    "Evaluate the trained models on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models if they were successfully trained\n",
    "test_dir = Path(PROCESSED_DATA_PATH) / 'test'\n",
    "test_images = list(test_dir.glob('**/*.png')) if test_dir.exists() else []\n",
    "\n",
    "print(f\"Found {len(test_images)} test images for evaluation\")\n",
    "\n",
    "if len(test_images) > 0:\n",
    "    # Prepare test data\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for img_path in test_images:\n",
    "        # Determine true label from path\n",
    "        true_label = 'authentic' if 'authentic' in str(img_path) else 'forged'\n",
    "        test_labels.append(true_label)\n",
    "    \n",
    "    print(f\"\\n📊 Test set composition:\")\n",
    "    test_label_counts = pd.Series(test_labels).value_counts()\n",
    "    print(test_label_counts)\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No test images available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c614cb1",
   "metadata": {},
   "source": [
    "## 7. Making Predictions <a id=\"predictions\"></a>\n",
    "\n",
    "Use the trained models to make predictions on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions with the traditional ML model if available\n",
    "ml_model_path = Path(MODELS_PATH) / 'random_forest_model.joblib'\n",
    "\n",
    "if ml_model_path.exists() and len(test_images) > 0:\n",
    "    print(\"Making predictions with Traditional ML model...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize predictor\n",
    "        ml_predictor = DocumentForgeryPredictor(str(ml_model_path), model_type='traditional_ml')\n",
    "        \n",
    "        # Make predictions on test images\n",
    "        ml_predictions = []\n",
    "        \n",
    "        for img_path in test_images[:10]:  # Limit to first 10 for demo\n",
    "            prediction = ml_predictor.predict_single_image(str(img_path))\n",
    "            prediction['image_path'] = str(img_path)\n",
    "            ml_predictions.append(prediction)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n✅ ML Predictions completed for {len(ml_predictions)} images:\")\n",
    "        \n",
    "        for i, pred in enumerate(ml_predictions[:5]):\n",
    "            img_name = Path(pred['image_path']).name\n",
    "            print(f\"   {i+1}. {img_name}: {pred['prediction']} (confidence: {pred['confidence']:.3f})\")\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = 0\n",
    "        for pred, true_label in zip(ml_predictions, test_labels[:len(ml_predictions)]):\n",
    "            if pred['prediction'] == true_label:\n",
    "                correct += 1\n",
    "        \n",
    "        ml_accuracy = correct / len(ml_predictions)\n",
    "        print(f\"\\n📊 ML Model Test Accuracy: {ml_accuracy:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ML prediction failed: {str(e)}\")\n",
    "        ml_predictions = []\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ ML model not available for predictions\")\n",
    "    ml_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca619b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions with CNN model if available\n",
    "cnn_model_path = Path(MODELS_PATH) / 'cnn_model.h5'\n",
    "\n",
    "if cnn_model_path.exists() and len(test_images) > 0:\n",
    "    print(\"Making predictions with CNN model...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize predictor\n",
    "        cnn_predictor = DocumentForgeryPredictor(str(cnn_model_path), model_type='cnn')\n",
    "        \n",
    "        # Make batch predictions\n",
    "        test_image_paths = [str(img) for img in test_images[:10]]  # Limit for demo\n",
    "        cnn_predictions = cnn_predictor.predict_batch(test_image_paths)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n✅ CNN Predictions completed for {len(cnn_predictions)} images:\")\n",
    "        \n",
    "        for i, pred in enumerate(cnn_predictions[:5]):\n",
    "            img_name = Path(pred['image_path']).name\n",
    "            print(f\"   {i+1}. {img_name}: {pred['prediction']} (confidence: {pred['confidence']:.3f})\")\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = 0\n",
    "        for pred, true_label in zip(cnn_predictions, test_labels[:len(cnn_predictions)]):\n",
    "            if pred['prediction'] == true_label:\n",
    "                correct += 1\n",
    "        \n",
    "        cnn_accuracy = correct / len(cnn_predictions)\n",
    "        print(f\"\\n📊 CNN Model Test Accuracy: {cnn_accuracy:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CNN prediction failed: {str(e)}\")\n",
    "        cnn_predictions = []\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ CNN model not available for predictions\")\n",
    "    cnn_predictions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43fd1c",
   "metadata": {},
   "source": [
    "## 8. Visualization and Analysis <a id=\"visualization\"></a>\n",
    "\n",
    "Create comprehensive visualizations of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4835e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions if we have any\n",
    "predictions_to_visualize = ml_predictions if ml_predictions else cnn_predictions\n",
    "\n",
    "if predictions_to_visualize and len(test_images) > 0:\n",
    "    print(\"Creating prediction visualizations...\")\n",
    "    \n",
    "    visualizer = DocumentForgeryVisualizer()\n",
    "    \n",
    "    # Visualize prediction confidence distribution\n",
    "    visualizer.plot_prediction_confidence_distribution(predictions_to_visualize)\n",
    "    \n",
    "    # Visualize images with predictions\n",
    "    image_paths = [pred['image_path'] for pred in predictions_to_visualize[:8]]\n",
    "    visualizer.visualize_image_predictions(\n",
    "        image_paths, \n",
    "        predictions_to_visualize[:8],\n",
    "        max_images=8\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No predictions available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix if we have enough predictions\n",
    "if len(predictions_to_visualize) >= 5:\n",
    "    # Prepare data for confusion matrix\n",
    "    y_true = test_labels[:len(predictions_to_visualize)]\n",
    "    y_pred = [pred['prediction'] for pred in predictions_to_visualize]\n",
    "    \n",
    "    # Convert to numeric labels for confusion matrix\n",
    "    label_map = {'authentic': 0, 'forged': 1}\n",
    "    y_true_numeric = [label_map[label] for label in y_true]\n",
    "    y_pred_numeric = [label_map[pred] for pred in y_pred]\n",
    "    \n",
    "    visualizer = DocumentForgeryVisualizer()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    visualizer.plot_confusion_matrix(\n",
    "        np.array(y_true_numeric),\n",
    "        np.array(y_pred_numeric),\n",
    "        class_names=['Authentic', 'Forged']\n",
    "    )\n",
    "    \n",
    "    # Plot classification report\n",
    "    visualizer.plot_classification_report(\n",
    "        np.array(y_true_numeric),\n",
    "        np.array(y_pred_numeric),\n",
    "        class_names=['Authentic', 'Forged']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Evaluation visualizations completed!\")\n",
    "else:\n",
    "    print(\"⚠️ Not enough predictions for confusion matrix (need at least 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b9357",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated a complete workflow for document forgery detection:\n",
    "\n",
    "### What we accomplished:\n",
    "1. ✅ **Data Setup**: Created/loaded document image dataset\n",
    "2. ✅ **Preprocessing**: Organized data into train/val/test splits\n",
    "3. ✅ **Feature Extraction**: Extracted comprehensive image features\n",
    "4. ✅ **Model Training**: Trained both traditional ML and deep learning models\n",
    "5. ✅ **Evaluation**: Assessed model performance on test data\n",
    "6. ✅ **Prediction**: Made predictions on new images\n",
    "7. ✅ **Visualization**: Created comprehensive analysis plots\n",
    "\n",
    "### To improve performance with real data:\n",
    "\n",
    "1. **Larger Dataset**: Use thousands of authentic and forged document images\n",
    "2. **Better Features**: Add domain-specific features like:\n",
    "   - EXIF metadata analysis\n",
    "   - Copy-move detection\n",
    "   - Splicing detection algorithms\n",
    "3. **Advanced Models**: Try:\n",
    "   - Transfer learning with pre-trained models\n",
    "   - Ensemble methods\n",
    "   - Attention-based networks\n",
    "4. **Data Augmentation**: Apply realistic document transformations\n",
    "5. **Cross-validation**: Use k-fold CV for robust evaluation\n",
    "\n",
    "### Real-world considerations:\n",
    "- **Adversarial attacks**: Test against sophisticated forgery methods\n",
    "- **Generalization**: Evaluate on different document types and sources\n",
    "- **Interpretability**: Provide explanations for predictions\n",
    "- **Performance**: Optimize for speed and memory usage\n",
    "- **Ethics**: Consider privacy and bias implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a03685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"📋 WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if sample_data:\n",
    "    print(f\"📁 Dataset: Synthetic sample data (100 images)\")\n",
    "else:\n",
    "    print(f\"📁 Dataset: Real data ({len(image_files)} images)\")\n",
    "\n",
    "if df_features is not None:\n",
    "    print(f\"🔧 Features: {len(df_features.columns)-2} extracted features\")\n",
    "\n",
    "if ml_detector and ml_results:\n",
    "    print(f\"🤖 ML Model: Random Forest (Test Accuracy: {ml_results.get('test_accuracy', 0):.3f})\")\n",
    "\n",
    "if cnn_detector and cnn_results:\n",
    "    print(f\"🧠 CNN Model: Custom CNN (Val Accuracy: {cnn_results.get('final_accuracy', 0):.3f})\")\n",
    "\n",
    "if predictions_to_visualize:\n",
    "    model_type = \"ML\" if ml_predictions else \"CNN\"\n",
    "    accuracy = ml_accuracy if ml_predictions else cnn_accuracy\n",
    "    print(f\"🎯 Predictions: {len(predictions_to_visualize)} images ({model_type} accuracy: {accuracy:.3f})\")\n",
    "\n",
    "print(\"\\n🎉 Document Forgery Detection workflow completed successfully!\")\n",
    "print(\"\\n💡 Next steps: Use real document data and experiment with different models for better performance.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
