"""
Advanced Model Training with Multiple Improvements:
1. More sophisticated feature extraction
2. Ensemble methods
3. Hyperparameter tuning
4. Class balancing techniques
5. Feature engineering
"""

import cv2
import numpy as np
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_classif
import joblib
from tqdm import tqdm
import sys
sys.path.append('src/features')
from build_features import DocumentFeatureExtractor

print("="*70)
print("ðŸš€ ADVANCED MODEL TRAINING - ACCURACY BOOST")
print("="*70)

# Load existing features if available
print("\nðŸ“Š Loading dataset...")
authentic_dir = Path('data/raw_synthetic/authentic')
forged_dir = Path('data/raw_synthetic/forged')

extractor = DocumentFeatureExtractor()

def extract_enhanced_features(img_path):
    """Extract enhanced features with additional discriminative features."""
    img = cv2.imread(str(img_path))
    if img is None:
        return None
    
    # Get base features
    features = extractor.extract_all_features(str(img_path))
    if not features:
        return None
    
    # Add additional advanced features
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    # 1. Frequency domain features (DCT)
    dct = cv2.dct(np.float32(gray))
    features['dct_mean'] = np.mean(dct)
    features['dct_std'] = np.std(dct)
    features['dct_max'] = np.max(dct)
    
    # 2. JPEG compression artifacts detection
    # Re-compress and measure difference
    _, encoded = cv2.imencode('.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 90])
    compressed = cv2.imdecode(encoded, cv2.IMREAD_COLOR)
    diff = cv2.absdiff(img, compressed)
    features['compression_diff_mean'] = np.mean(diff)
    features['compression_diff_std'] = np.std(diff)
    
    # 3. Noise analysis (high-frequency components)
    median = cv2.medianBlur(gray, 5)
    noise = cv2.absdiff(gray, median)
    features['noise_level'] = np.mean(noise)
    features['noise_variance'] = np.var(noise)
    
    # 4. Edge consistency
    edges_canny = cv2.Canny(gray, 50, 150)
    # Dilate edges and check consistency
    kernel = np.ones((3,3), np.uint8)
    dilated = cv2.dilate(edges_canny, kernel, iterations=1)
    features['edge_consistency'] = np.sum(dilated) / (dilated.shape[0] * dilated.shape[1])
    
    # 5. Contrast inconsistencies (local vs global)
    # Divide image into blocks and measure contrast variation
    h, w = gray.shape
    block_size = 32
    contrasts = []
    for i in range(0, h - block_size, block_size):
        for j in range(0, w - block_size, block_size):
            block = gray[i:i+block_size, j:j+block_size]
            contrasts.append(np.std(block))
    
    features['contrast_variation'] = np.std(contrasts) if contrasts else 0
    features['contrast_mean'] = np.mean(contrasts) if contrasts else 0
    
    # 6. Color histogram inconsistencies
    if len(img.shape) == 3:
        color_hist = []
        for channel in range(3):
            hist = cv2.calcHist([img], [channel], None, [256], [0, 256])
            color_hist.extend(hist.flatten())
        
        features['color_hist_entropy'] = -np.sum(
            (np.array(color_hist) + 1e-10) * np.log2(np.array(color_hist) + 1e-10)
        )
    
    return features

print("\nðŸ”¬ Extracting enhanced features...")
X = []
y = []
filenames = []

# Process authentic images (increase sample size)
print("\nðŸ“Š Processing authentic images...")
authentic_imgs = list(authentic_dir.glob('*.jpg'))[:1500]
for img_path in tqdm(authentic_imgs, desc="Authentic features"):
    features = extract_enhanced_features(img_path)
    if features:
        feature_values = [v for k, v in features.items() 
                         if k not in ['filename', 'filepath'] and isinstance(v, (int, float))]
        X.append(feature_values)
        y.append(0)
        filenames.append(img_path.name)

print(f"Loaded {len([i for i in y if i == 0])} authentic samples")

# Process forged images
print("\nðŸ“Š Processing forged images...")
forged_imgs = list(forged_dir.glob('*.jpg'))[:1500]
for img_path in tqdm(forged_imgs, desc="Forged features"):
    features = extract_enhanced_features(img_path)
    if features:
        feature_values = [v for k, v in features.items() 
                         if k not in ['filename', 'filepath'] and isinstance(v, (int, float))]
        X.append(feature_values)
        y.append(1)
        filenames.append(img_path.name)

print(f"Loaded {len([i for i in y if i == 1])} forged samples")

X = np.array(X)
y = np.array(y)

print(f"\nâœ… Total samples: {len(X)}")
print(f"   Features: {X.shape[1]}")
print(f"   Authentic: {np.sum(y == 0)}")
print(f"   Forged: {np.sum(y == 1)}")

# Split data with stratification
print("\nðŸ“Š Splitting dataset (75% train, 25% test)...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

print(f"   Training: {len(X_train)} samples")
print(f"   Testing: {len(X_test)} samples")

# Feature scaling
print("\nâš–ï¸  Scaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Feature selection (select best features)
print("\nðŸ” Selecting best features...")
selector = SelectKBest(score_func=f_classif, k=25)  # Select top 25 features
X_train_selected = selector.fit_transform(X_train_scaled, y_train)
X_test_selected = selector.transform(X_test_scaled)

print(f"   Selected {X_train_selected.shape[1]} best features")

# Train multiple models
print("\n" + "="*70)
print("ðŸŽ¯ TRAINING ENSEMBLE OF MODELS")
print("="*70)

# 1. Random Forest with optimized parameters
print("\n1ï¸âƒ£  Training Random Forest...")
rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=40,
    min_samples_split=3,
    min_samples_leaf=1,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'  # Handle class imbalance
)
rf.fit(X_train_selected, y_train)
rf_score = rf.score(X_test_selected, y_test)
print(f"   Random Forest accuracy: {rf_score:.4f}")

# 2. Gradient Boosting
print("\n2ï¸âƒ£  Training Gradient Boosting...")
gb = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=7,
    min_samples_split=3,
    min_samples_leaf=1,
    random_state=42
)
gb.fit(X_train_selected, y_train)
gb_score = gb.score(X_test_selected, y_test)
print(f"   Gradient Boosting accuracy: {gb_score:.4f}")

# 3. SVM with RBF kernel
print("\n3ï¸âƒ£  Training SVM...")
svm = SVC(
    C=10,
    gamma='scale',
    kernel='rbf',
    probability=True,
    random_state=42,
    class_weight='balanced'
)
svm.fit(X_train_selected, y_train)
svm_score = svm.score(X_test_selected, y_test)
print(f"   SVM accuracy: {svm_score:.4f}")

# 4. Logistic Regression
print("\n4ï¸âƒ£  Training Logistic Regression...")
lr = LogisticRegression(
    C=1.0,
    max_iter=1000,
    random_state=42,
    class_weight='balanced'
)
lr.fit(X_train_selected, y_train)
lr_score = lr.score(X_test_selected, y_test)
print(f"   Logistic Regression accuracy: {lr_score:.4f}")

# 5. Voting Classifier (Ensemble)
print("\n5ï¸âƒ£  Creating Voting Ensemble...")
voting = VotingClassifier(
    estimators=[
        ('rf', rf),
        ('gb', gb),
        ('svm', svm),
        ('lr', lr)
    ],
    voting='soft',  # Use probability voting
    weights=[2, 2, 1, 1]  # Give more weight to RF and GB
)
voting.fit(X_train_selected, y_train)
voting_score = voting.score(X_test_selected, y_test)
print(f"   Voting Ensemble accuracy: {voting_score:.4f}")

# Evaluate best model
print("\n" + "="*70)
print("ðŸ“Š DETAILED EVALUATION OF VOTING ENSEMBLE")
print("="*70)

y_pred = voting.predict(X_test_selected)
y_pred_proba = voting.predict_proba(X_test_selected)

test_acc = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])

print(f"\nðŸŽ¯ Testing Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"ðŸ“ˆ ROC AUC Score: {roc_auc:.4f}")

print("\nðŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred, 
                          target_names=['Authentic', 'Forged'],
                          digits=4))

print("\nðŸ“Š Confusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(f"\nâœ… True Negatives (Authentic correctly identified): {cm[0][0]}")
print(f"âš ï¸  False Positives (Authentic wrongly flagged): {cm[0][1]}")
print(f"âš ï¸  False Negatives (Forged missed): {cm[1][0]}")
print(f"âœ… True Positives (Forged correctly detected): {cm[1][1]}")

# Calculate specific metrics
precision_forged = cm[1][1] / (cm[1][1] + cm[0][1]) if (cm[1][1] + cm[0][1]) > 0 else 0
recall_forged = cm[1][1] / (cm[1][1] + cm[1][0]) if (cm[1][1] + cm[1][0]) > 0 else 0
f1_forged = 2 * (precision_forged * recall_forged) / (precision_forged + recall_forged) if (precision_forged + recall_forged) > 0 else 0

print(f"\nðŸŽ¯ Forged Document Detection:")
print(f"   Precision: {precision_forged:.4f} ({precision_forged*100:.2f}%)")
print(f"   Recall: {recall_forged:.4f} ({recall_forged*100:.2f}%)")
print(f"   F1-Score: {f1_forged:.4f}")

# Cross-validation (using n_jobs=1 to avoid Windows multiprocessing issues)
print("\nðŸ”„ Cross-validation (5-fold)...")
cv_scores = cross_val_score(voting, X_train_selected, y_train, cv=5, n_jobs=1)
print(f"   CV Scores: {cv_scores}")
print(f"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Save the best model
print("\nðŸ’¾ Saving models...")
joblib.dump(voting, 'models/ensemble_voting_model.joblib')
joblib.dump(scaler, 'models/scaler_ensemble.joblib')
joblib.dump(selector, 'models/feature_selector.joblib')

# Also save individual models
joblib.dump(rf, 'models/random_forest_optimized.joblib')
joblib.dump(gb, 'models/gradient_boosting.joblib')
joblib.dump(svm, 'models/svm_model.joblib')

print("\nâœ… All models saved!")
print("   - models/ensemble_voting_model.joblib (BEST)")
print("   - models/random_forest_optimized.joblib")
print("   - models/gradient_boosting.joblib")
print("   - models/svm_model.joblib")
print("   - models/scaler_ensemble.joblib")
print("   - models/feature_selector.joblib")

print("\n" + "="*70)
print("ðŸŽ‰ TRAINING COMPLETE!")
print("="*70)

print("\nðŸ“Š ACCURACY PROGRESSION:")
print(f"   1st model (augmented forgeries):  45.02%")
print(f"   2nd model (synthetic forgeries):  76.17%")
print(f"   3rd model (ensemble + features):  {test_acc*100:.2f}%")
print(f"\n   Total improvement: {(test_acc - 0.4502)*100:.2f} percentage points! ðŸš€")
